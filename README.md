# ğŸ“š Document-Grounded Study Assistant

This web app helps students study more effectively by querying uploaded class notes, readings, or PDFs. Unlike generic chatbots, it **never hallucinates**â€”if a question falls outside the scope of the provided documents, it responds with â€œNo responseâ€ or â€œCanâ€™t answer.â€ The result is a trusted, context-grounded Q&A tool built with **Flask**, **React**, and **Vellum AI**.

---

## âš™ï¸ How Vellum Was Used

The backend intelligence is fully orchestrated by Vellum:

### ğŸ§  RAG Workflow & CLI Integration
- Built a **Retrieval-Augmented Generation (RAG)** pipeline using Vellumâ€™s no-code **Workflow builder**, combining search, prompt, branching, and output nodes.
- Integrated the entire workflow into the Flask backend using the **Vellum CLI**, allowing seamless code export and deployment.

### ğŸ“„ Document Ingestion & Retrieval
- Users upload PDFs or notes via `/api/upload-documents`.
- Documents are automatically segmented, embedded, and indexed using Vellum's API.
- A **Search Node** retrieves relevant chunks based on the user's query at runtime.

### ğŸ“ Prompted, Grounded Generation
- Retrieved content is passed through a **Prompt Node**, which generates a response strictly based on that context.
- If no relevant context is found, the response is safely declined to avoid hallucinations.

### ğŸ§ª Evaluation & Monitoring
- Vellumâ€™s built-in evaluation suite was used to monitor retrieval accuracy, faithfulness, and output correctness.
- Integrated logs, latency metrics, and versioning support for ongoing improvement and observability.

---

## âœ¨ Why Vellum?

- **No/Low-Code with Full Control** â€“ Visually build and edit workflows, then export them to production-ready code.
- **Flexible Model Integration** â€“ Supports OpenAI, Anthropic, Cohere, and other providers.
- **Grounded Answers** â€“ Ensures answers are always tied to uploaded documents, with fallback logic that avoids hallucinations.
- **Powerful Evaluation Tools** â€“ Built-in support for testing, scoring, and improving RAG pipeline quality.
- **Agentic Orchestration** â€“ Create advanced flows with retries, conditional logic, and custom decision trees.
- **Observability** â€“ Provides insights, logs, versioning, and latency tracking to improve reliability.

---

## ğŸ§© Tech Stack

| Component     | Role                                                                 |
|--------------|----------------------------------------------------------------------|
| **Flask**     | REST API for uploading documents and querying questions              |
| **React**     | Frontend UI for file uploads and question submission                 |
| **Vellum AI** | Handles all retrieval, generation, and evaluation behind the scenes  |

---

## ğŸ¥ Demo

Click below to watch a short demo of the app in action:

[![Video Demo](https://github.com/user-attachments/assets/e277e8b3-63e3-4aef-bbe8-8bae528f643f)](https://github.com/user-attachments/assets/e277e8b3-63e3-4aef-bbe8-8bae528f643f)

---

## ğŸ§  Summary

Vellum AI transformed what wouldâ€™ve been weeks of manual orchestration and LLM pipeline code into a fast, reliable, and maintainable AI backend. It helped speed up development dramatically while enforcing guardrails that prevent misinformation. With Vellumâ€™s flexibility, observability, and powerful tools, this project was able to stay lightweight while delivering trustworthy AI results.

---


https://github.com/user-attachments/assets/e277e8b3-63e3-4aef-bbe8-8bae528f643f
